# Manually set your working directory to ONR_MBAP Folder
# Use the blue gear to do this
# Make sure you have your symbolic links prepared
SharePoint1 = "SharePoint/Documents - UH-ONR/"
SharePoint2 = "SharePoint/General - UH ONR Scheduling/"
TimesServer = "TimesServer/"
R_Pathways = "Scoring/"
GitHub = "~/GitHub/ONR_MBAP"
# Load in Packages
library(tidyverse)
library(readxl)
library(lubridate)
library(openxlsx)
# Load in the data
Nback.original <- read.csv(paste0(TimesServer,"MODIFIED_DS/VR/Nback.day.type.added.csv"))
# Load in the data
Nback.original <- read.csv(paste0(TimesServer,"MODIFIED_DS/VR/Nback.day.type.added.csv"))
TimesServer
# This is the main script for the ONR Behavioral Data scoring
# Manually set your working directory to ONR_MBAP Folder
# Use the blue gear to do this
# Make sure you have your symbolic links prepared
SharePoint1 = "SharePoint/Documents - UH-ONR/"
SharePoint2 = "SharePoint/General - UH ONR Scheduling/"
TimesServer = "TimesServer/"
R_Pathways = "Scoring/"
GitHub = "~/GitHub/ONR_MBAP"
##################################################################################
########################## The Rest is Automatic #################################
##################################################################################
# Load the package
library(tidyverse)
library(qualtRics)
library(readxl)
library(readr)
library(openxlsx)
# Set working directory
setwd(GitHub)
# This is the script for scoring Nback VR Data
# Load in Packages
library(tidyverse)
library(readxl)
library(lubridate)
library(openxlsx)
# Load in the data
Nback.original <- read.csv(paste0(TimesServer,"MODIFIED_DS/VR/Nback.day.type.added.csv"))
# Rename UserID to ID
Nback.original <- rename(Nback.original, ID = UserID)
# Remove the spaces in the variable Day
Nback.original <- mutate(Nback.original, Day = gsub(" ", "", Day))
# Remove Day 0 from our dataset
Nback.removed.days <- Nback.original %>%
filter(!(Day %in% c("Day0")))
# DELETE ME
OneParticipant <- Nback.removed.days %>%
filter(ID == 2)
xtabs(~BlockName + Day, OneParticipant)
# DELETE ME
# Keep only the 4 conditions of interest
Nback.4.conditions <- Nback.removed.days %>%
filter(BlockName %in% c("2-Back AR w ND",
"2-Back AR w D",
"2-Back RAR w ND",
"2-Back RAR w D"))
# Add a Trial.Type variable
all.Nback.sessions <- list()
# Create a 'file.name' for file extraction
Nback.file.names <- Nback.4.conditions %>% select(ID, Day) %>% group_by(ID, Day) %>% unique()
# Run the loop
for(ii in 1:nrow(Nback.file.names)) {
# Extract the ID in the list
current.ID <- Nback.file.names$ID[ii]
# Extract the  Day Type in the list
current.Day.Type <- Nback.file.names$Day[ii]
# Use ID and Day Type to subset one CSV file from the Nback
current.Nback <- Nback.4.conditions %>%
filter(ID == current.ID & Day == current.Day.Type)
# Split the Figure variable to obtain the sequence of shown stimuli and save that as a separate data frame
split.Figure.df <- str_split(current.Nback$Figure, pattern = "_") %>%
do.call(rbind,.) %>%
data.frame() %>%
select(-X3)
# Give them names associated with what is being measure
names(split.Figure.df) <- c("Shape", "Sequence")
# Introduce them into the main Nback dataframe
current.Nback2 <- cbind(current.Nback, split.Figure.df)
# Add the second variable that duplicates ItemID and shifts it down two rows
current.Nback2$ItemID2 <- c("-", "-", current.Nback2$ItemID[1:(length(current.Nback2$ItemID)-2)])
# Use the second variable to identify target trials by marking matching rows as 'Target
current.Nback2 <- current.Nback2 %>%
mutate(Trial.Type = case_when(
ItemID == ItemID2 & Sequence %in% c(2:19) ~ "Target",
ItemID != ItemID2 & Sequence %in% c(2:19) ~ "Non-Target",
TRUE ~ "FirstSecond Trial"
)) %>%
select(ID, BlockID, ItemID, ItemID2, Trial.Type, everything())
# Save this manipulation into the list
all.Nback.sessions[[ii]] <- current.Nback2
}
# Save the list with the transformation back as a dataframe
Nback.trial.type <- all.Nback.sessions%>%
do.call(rbind,.) %>%
data.frame()
# Quality Control
prop.table(table(Nback.trial.type$Trial.Type))
# Create a dataset that has enough information to do a binary analysis
Nback_binary <- select(Nback.trial.type, ID, BlockName, Trial.Type, Day, Correct)
# Create a data frame with N-back performance based on performance for the correct variable
Nback.grouped.Trial.Type <- Nback.trial.type %>%
group_by(ID, BlockName, Trial.Type, Day) %>%
summarize(trial.mean.correct = round(mean(Correct),2),
trial.num = length(Correct)) %>%
arrange(ID)
# Create another data frame that captures reaction time information for the Nbacl
Nback.grouped.Reaction.Time <- Nback.trial.type %>%
group_by(ID,Trial.Type, Correct, BlockName, Day) %>%
summarize(mean.RT = mean(TimeToImpact)) %>%
arrange(ID)
Nback.grouped.Reaction.Time.wide <- Nback.grouped.Reaction.Time %>%
mutate(Correct = ifelse(Correct == 1, "Correct", "Incorrect")) %>%
pivot_wider(names_from = c(Correct, Trial.Type),
values_from = c(mean.RT))
names(Nback.grouped.Reaction.Time.wide) <- c(names(Nback.grouped.Reaction.Time.wide)[1:3],
paste0(names(Nback.grouped.Reaction.Time.wide)[4:9],"_ms"))
# NA's come from that event not occuring at all because the participant was either too good or bad
sapply(Nback.grouped.Reaction.Time.wide, function(x) sum(is.na(x)))
### Additionally- we are going to save this data frame to the Nback as a reaction to a target trial being correct
# This means that it ignores trials being wrong by not hitting with the arrows.
Nback.grouped.Reaction.Target <- Nback.trial.type %>%
group_by(ID, BlockName, Trial.Type, Day) %>%
mutate(target.mean.reacted = round(mean(Reacted),2),
trial.num = length(Reacted)) %>% head %>% view()
ungroup() %>%
select(ID, BlockName, Trial.Type, Day, target.mean.reacted, trial.num) %>%
unique() %>%
arrange(ID)
Nback.trial.type %>%
group_by(ID, BlockName, Trial.Type, Day) %>%
mutate(target.mean.reacted = round(mean(Reacted),2),
trial.num = length(Reacted)) %>% head() %>% view()
names(Nback.grouped.Reaction.Target)
unique(Nback.trial.type$Trial.Type)
Nback.trial.type %>%
mutate(Correct_2 = case_when(
Trial.Type == "FirstSecond Trial" & Reacted == 0 ~ 1,
Trial.Type == "Target" & Reacted == 1 ~ 1,
Trial.Type == "Non-Target" & Reacted  == 0 ~ 1,
TRUE ~ 0
)) %>%
group_by(ID, BlockName, Trial.Type, Day, Correct_2) %>%
mutate(mean.RT = mean(TimeToImpact),
trial.num = length(Reacted))
Nback.trial.type %>%
mutate(Correct_2 = case_when(
Trial.Type == "FirstSecond Trial" & Reacted == 0 ~ 1,
Trial.Type == "Target" & Reacted == 1 ~ 1,
Trial.Type == "Non-Target" & Reacted  == 0 ~ 1,
TRUE ~ 0
)) %>%
group_by(ID, BlockName, Trial.Type, Day, Correct_2) %>%
summarise(mean.RT = mean(TimeToImpact),
trial.num = length(Reacted))
Nback.trial.type %>%
mutate(Correct_2 = case_when(
Trial.Type == "FirstSecond Trial" & Reacted == 0 ~ 1,
Trial.Type == "Target" & Reacted == 1 ~ 1,
Trial.Type == "Non-Target" & Reacted  == 0 ~ 1,
TRUE ~ 0
)) %>%
group_by(ID, BlockName, Trial.Type, Day) %>%
summarise(trial.mean.correct = mean(Correct_2),
trial.num = length(Reacted))
### Additionally- we are going to save this data frame to the Nback as a reaction to a target trial being correct
# This means that it ignores trials being wrong by not hitting with the arrows.
Nback.grouped.Reaction.Target <- Nback.trial.type %>%
mutate(Correct_2 = case_when(
Trial.Type == "FirstSecond Trial" & Reacted == 0 ~ 1,
Trial.Type == "Target" & Reacted == 1 ~ 1,
Trial.Type == "Non-Target" & Reacted  == 0 ~ 1,
TRUE ~ 0
)) %>%
group_by(ID, BlockName, Trial.Type, Day) %>%
summarise(trial.mean.correct = mean(Correct_2),
trial.num = length(Reacted))
Nback.grouped.Reaction.Target.wid <- Nback.grouped.Reaction.Target %>%
pivot_wider(names_from = Trial.Type, values_from = c(trial.num, trial.mean.correct))
Nback.grouped.Reaction.Target.wide <- Nback.grouped.Reaction.Target %>%
pivot_wider(names_from = Trial.Type, values_from = c(trial.num, trial.mean.correct))
view(head(Nback.grouped.Reaction.Target.wide))
### Additionally- we are going to save this data frame to the Nback as a reaction to a target trial being correct
# This means that it ignores trials being wrong by not hitting with the arrows.
Nback.grouped.Reaction.Target <- Nback.trial.type %>%
mutate(Correct_2 = case_when(
Trial.Type == "FirstSecond Trial" & Reacted == 0 ~ 1,
Trial.Type == "Target" & Reacted == 1 ~ 1,
Trial.Type == "Non-Target" & Reacted  == 0 ~ 1,
TRUE ~ 0
)) %>%
group_by(ID, BlockName, Trial.Type, Day) %>%
summarise(trial.mean.correct = round(mean(Correct_2),2),
trial.num = round(length(Reacted),2))
Nback.grouped.Reaction.Target.wide <- Nback.grouped.Reaction.Target %>%
pivot_wider(names_from = Trial.Type, values_from = c(trial.num, trial.mean.correct))
# This is the main script for the ONR Behavioral Data scoring
# Manually set your working directory to ONR_MBAP Folder
# Use the blue gear to do this
# Make sure you have your symbolic links prepared
SharePoint1 = "SharePoint/Documents - UH-ONR/"
SharePoint2 = "SharePoint/General - UH ONR Scheduling/"
TimesServer = "TimesServer/"
R_Pathways = "Scoring/"
GitHub = "~/GitHub/ONR_MBAP"
##################################################################################
########################## The Rest is Automatic #################################
##################################################################################
# Load the package
library(tidyverse)
library(qualtRics)
library(readxl)
library(readr)
library(openxlsx)
# Set working directory
setwd(GitHub)
# This is the script for scoring Nback VR Data
# Load in Packages
library(tidyverse)
library(readxl)
library(lubridate)
library(openxlsx)
# Load in the data
Nback.original <- read.csv(paste0(TimesServer,"MODIFIED_DS/VR/Nback.day.type.added.csv"))
# Rename UserID to ID
Nback.original <- rename(Nback.original, ID = UserID)
# Remove the spaces in the variable Day
Nback.original <- mutate(Nback.original, Day = gsub(" ", "", Day))
# Remove Day 0 from our dataset
Nback.removed.days <- Nback.original %>%
filter(!(Day %in% c("Day0")))
# DELETE ME
OneParticipant <- Nback.removed.days %>%
filter(ID == 2)
xtabs(~BlockName + Day, OneParticipant)
# DELETE ME
# Keep only the 4 conditions of interest
Nback.4.conditions <- Nback.removed.days %>%
filter(BlockName %in% c("2-Back AR w ND",
"2-Back AR w D",
"2-Back RAR w ND",
"2-Back RAR w D"))
# Add a Trial.Type variable
all.Nback.sessions <- list()
# Create a 'file.name' for file extraction
Nback.file.names <- Nback.4.conditions %>% select(ID, Day) %>% group_by(ID, Day) %>% unique()
# Run the loop
for(ii in 1:nrow(Nback.file.names)) {
# Extract the ID in the list
current.ID <- Nback.file.names$ID[ii]
# Extract the  Day Type in the list
current.Day.Type <- Nback.file.names$Day[ii]
# Use ID and Day Type to subset one CSV file from the Nback
current.Nback <- Nback.4.conditions %>%
filter(ID == current.ID & Day == current.Day.Type)
# Split the Figure variable to obtain the sequence of shown stimuli and save that as a separate data frame
split.Figure.df <- str_split(current.Nback$Figure, pattern = "_") %>%
do.call(rbind,.) %>%
data.frame() %>%
select(-X3)
# Give them names associated with what is being measure
names(split.Figure.df) <- c("Shape", "Sequence")
# Introduce them into the main Nback dataframe
current.Nback2 <- cbind(current.Nback, split.Figure.df)
# Add the second variable that duplicates ItemID and shifts it down two rows
current.Nback2$ItemID2 <- c("-", "-", current.Nback2$ItemID[1:(length(current.Nback2$ItemID)-2)])
# Use the second variable to identify target trials by marking matching rows as 'Target
current.Nback2 <- current.Nback2 %>%
mutate(Trial.Type = case_when(
ItemID == ItemID2 & Sequence %in% c(2:19) ~ "Target",
ItemID != ItemID2 & Sequence %in% c(2:19) ~ "Non-Target",
TRUE ~ "FirstSecond Trial"
)) %>%
select(ID, BlockID, ItemID, ItemID2, Trial.Type, everything())
# Save this manipulation into the list
all.Nback.sessions[[ii]] <- current.Nback2
}
# Save the list with the transformation back as a dataframe
Nback.trial.type <- all.Nback.sessions%>%
do.call(rbind,.) %>%
data.frame()
# Quality Control
prop.table(table(Nback.trial.type$Trial.Type))
# Create a dataset that has enough information to do a binary analysis
Nback_binary <- select(Nback.trial.type, ID, BlockName, Trial.Type, Day, Correct)
# Create a data frame with N-back performance based on performance for the correct variable
Nback.grouped.Trial.Type <- Nback.trial.type %>%
group_by(ID, BlockName, Trial.Type, Day) %>%
summarize(trial.mean.correct = round(mean(Correct),2),
trial.num = length(Correct)) %>%
arrange(ID)
# Convert Nback trial accuracy to wide
Nback.grouped.Trial.Type.wide <- Nback.grouped.Trial.Type %>%
pivot_wider(names_from = Trial.Type, values_from = c(trial.num, trial.mean.correct))
# Create another data frame that captures reaction time information for the Nbacl
Nback.grouped.Reaction.Time <- Nback.trial.type %>%
group_by(ID,Trial.Type, Correct, BlockName, Day) %>%
summarize(mean.RT = mean(TimeToImpact)) %>%
arrange(ID)
Nback.grouped.Reaction.Time.wide <- Nback.grouped.Reaction.Time %>%
mutate(Correct = ifelse(Correct == 1, "Correct", "Incorrect")) %>%
pivot_wider(names_from = c(Correct, Trial.Type),
values_from = c(mean.RT))
names(Nback.grouped.Reaction.Time.wide) <- c(names(Nback.grouped.Reaction.Time.wide)[1:3],
paste0(names(Nback.grouped.Reaction.Time.wide)[4:9],"_ms"))
# NA's come from that event not occuring at all because the participant was either too good or bad
sapply(Nback.grouped.Reaction.Time.wide, function(x) sum(is.na(x)))
### Additionally- we are going to save this data frame to the Nback as a reaction to a target trial being correct
# This means that it ignores trials being wrong by not hitting with the arrows.
Nback.grouped.Reaction.Target <- Nback.trial.type %>%
mutate(Correct_2 = case_when(
Trial.Type == "FirstSecond Trial" & Reacted == 0 ~ 1,
Trial.Type == "Target" & Reacted == 1 ~ 1,
Trial.Type == "Non-Target" & Reacted  == 0 ~ 1,
TRUE ~ 0
)) %>%
group_by(ID, BlockName, Trial.Type, Day) %>%
summarise(trial.mean.correct = round(mean(Correct_2),2),
trial.num = round(length(Reacted),2))
Nback.grouped.Reaction.Target.wide <- Nback.grouped.Reaction.Target %>%
pivot_wider(names_from = Trial.Type, values_from = c(trial.num, trial.mean.correct))
# Save the data
write_csv(x = Nback.grouped.Trial.Type.wide,
file = paste0(TimesServer,"FINAL_DS/VR/NbackAccuracyWide.csv"))
write_csv(x = Nback.grouped.Reaction.Time.wide,
file = paste0(TimesServer,"FINAL_DS/VR/NbackReactionTimeWide.csv")) # fixed; NA's are a good thing
write_csv(x = Nback.grouped.Reaction.Target.wide,
file = paste0(TimesServer,"FINAL_DS/VR/NbackTargetReacted.csv")) # Needs to be fixed
write_csv(x = Nback_binary,
file = paste0(TimesServer,"FINAL_DS/VR/NbackAccuracyLong_Binary.csv"))
setwd("//files.times.uh.edu/labs/Grigorenko_ONR-MPAB/MAIN_STUDY/Data/FINAL_DS/VR")
setwd("//files.times.uh.edu/labs/Grigorenko_ONR-MPAB/MAIN_STUDY/Data/FINAL_DS/VR")
dat <- read.csv("NbackAccuracyWide.csv")
glimpse(dat)
table(dat$trial.num)
sapply(dat, function(x), table(x))
sapply(dat, function(x) table(x))
view(dat)
setwd("//files.times.uh.edu/labs/Grigorenko_ONR-MPAB/MAIN_STUDY/Data/FINAL_DS/VR")
dat <- read.csv("NbackTargetReactedWide.csv")
sapply(dat, function(x) table(x))
view(dat)
dat <- read.csv("NbackReactionTimeWide.csv")
sapply(dat, function(x) table(x))
view(dat)
# Instructions:
# Run the first 23 lines of code
# After data have been imported and scored, run the first 20 lines of code
# Run the remaining lines of code, manually score, then rerun the remaining again
# Set the location for your working directory (Where your scripts are saved)
WorkingDirectory <- "C:/Users/lledesma.TIMES/Documents/GitHub/KBB/"
# Set the location for where your data is saved
DataLocation <- "C:/Users/lledesma.TIMES/Documents/KBB/Data/"
# Set the pathway for the final screener data
FinalData_PW <- paste0(DataLocation,"FINAL_DS/Screener/")
# Set the pathway to the final matched sibling data
MatchedSibling_PW <- paste0(DataLocation,"FINAL_DS/Screener/Matched_Siblings/")
# Set the working directory
setwd(WorkingDirectory)
# Run the main script that uses API to download the data and score it
source("MainScripts/MainScript_ImportingAllDatafromKoboToolBoxandScoringIt.R")
# Load in each dataset (All Recently Scored)
Incorrect.Screeners <- read_excel(paste0(FinalData_PW, "1) Incorrect Screeners (level 1).xlsx"))
# Instructions:
# Run the first 23 lines of code
# After data have been imported and scored, run the first 20 lines of code
# Run the remaining lines of code, manually score, then rerun the remaining again
# Set the location for your working directory (Where your scripts are saved)
WorkingDirectory <- "C:/Users/lledesma.TIMES/Documents/GitHub/KBB/"
# Set the location for where your data is saved
DataLocation <- "C:/Users/lledesma.TIMES/Documents/KBB/Data/"
# Set the pathway for the final screener data
FinalData_PW <- paste0(DataLocation,"FINAL_DS/Screener/")
# Set the pathway to the final matched sibling data
MatchedSibling_PW <- paste0(DataLocation,"FINAL_DS/Screener/Matched_Siblings/")
# Set the working directory
setwd(WorkingDirectory)
# Remove all global environment objects to declutter
#rm(list=ls())
# Load in each dataset (All Recently Scored)
Incorrect.Screeners <- read_excel(paste0(FinalData_PW, "1) Incorrect Screeners (level 1).xlsx"))
Excluded.Children <- read_excel(paste0(FinalData_PW, "2) Excluded Children (level 1).xlsx"))
HOH.No.Matches.unnested <- read_excel(paste0(FinalData_PW, "3) HOH No Matches (level 1).xlsx"))
HOH.Potential.Matches.unnested <- read_excel(paste0(FinalData_PW, "4) HOH Potential Matches (level 1).xlsx"))
# Load in the IDs that have been categoriazed in Final_ID_Tracker.xlsx
Siblings <- read_excel(paste0(MatchedSibling_PW,"Final_ID_Tracker.xlsx"), sheet= "Siblings")
Half_Siblings <- read_excel(paste0(MatchedSibling_PW,"Final_ID_Tracker.xlsx"), sheet= "Half-Siblings")
Other <- read_excel(paste0(MatchedSibling_PW,"Final_ID_Tracker.xlsx"), sheet= "Other")
# Combine these ID's into accounted for IDs
accounted.for.IDs <- c(Siblings$Child_ID, Half_Siblings$Child_ID, Other$Child_ID)
# Remove rows from these datasets if the Child_ID is present in the Final_ID_Dataset
Incorrect.Screeners.final <- Incorrect.Screeners %>%
filter(!(Child_ID %in% accounted.for.IDs))
Excluded.Children.final <- Excluded.Children %>%
filter(!(Child_ID %in% accounted.for.IDs))
HOH.No.Matches.unnested.final <- HOH.No.Matches.unnested %>%
filter(!(Child_ID %in% accounted.for.IDs))
HOH.Potential.Matches.unnested.final <- HOH.Potential.Matches.unnested %>%
filter(!(Child_ID %in% accounted.for.IDs))
# Save the partialled out HOH Potential Matches
HOH.Potential.Matches.unnested.final.shorted <- HOH.Potential.Matches.unnested.final %>%
select(HOH_ID, Name_of_the_Village, Date_of_Evaluation, HOH_First_Name, HOH_Last_Name, Respondant_First_Name, Respondant_Last_Name, Respondant_relationship, BF, BM, Child_First_Name, Child_Last_Name, Child_Date_of_Birth, Child_age, Child_Gender, Epilepsy, KBB_DD_status, Child_ID, Overall.Summary,
EEG_Group:EEG_Exc_Rea) %>%
arrange(HOH_ID)
# Minor data cleaning (reading data creates a time for date variables)
HOH.Potential.Matches.unnested.final.shorted$Date_of_Evaluation <- as.character(HOH.Potential.Matches.unnested.final.shorted$Date_of_Evaluation)
HOH.Potential.Matches.unnested.final.shorted$Child_Date_of_Birth <- as.character(HOH.Potential.Matches.unnested.final.shorted$Child_Date_of_Birth)
# Save the dataset
write.xlsx(list(data = HOH.Potential.Matches.unnested.final.shorted), file =  paste0(MatchedSibling_PW,"Subjects_that_need_an_ID.xlsx"))
# Creating a final dataset
Manual.Groupings <- rbind(Siblings,
Half_Siblings,
Other)
# Bind the mutually exclusive datasets back into one
Reconstructed.data.final <- rbind(Incorrect.Screeners.final,
Excluded.Children.final,
HOH.No.Matches.unnested.final,
HOH.Potential.Matches.unnested.final)
# Give it an empty variable that represents ID
Reconstructed.data.final$ID <- NA
# Convert the DOE into character
Reconstructed.data.final$Date_of_Evaluation <- as.character(Reconstructed.data.final$Date_of_Evaluation)
# Keep the same variables in the same order as ID. Tracker
Reconstructed.data.final <- Reconstructed.data.final[, c(names(Manual.Groupings))]
# Merge everything into one Final Dataset
Final.Data <- rbind(Manual.Groupings,
Reconstructed.data.final)
# Set the order of the overall summary
Final.Data <- Final.Data %>%
mutate(Overall.Summary = factor(Overall.Summary,
levels = c("Incorrect Screener",
"Excluded Children",
"No Matches Within HOH",
"Recruited With Incorrect Screener",
"Control for Incorrect Screener",
"Should have been Excluded",
"Control for Excluded Child",
"Manual: Unable to Match",
"Manual: Matched Half-Siblings",
"Manual: Matched Siblings")))
# Get an overview of the dataset
data.frame(Frequency = cbind(table(Final.Data$Overall.Summary)))
# Save the data
write.xlsx(list(data = Final.Data), file =  paste0(FinalData_PW, "Comprehensive Screener Scoring.xlsx"))
# Quick modification
Siblings.DOE.arranged <- read_excel(paste0(MatchedSibling_PW, "Final_ID_Tracker.xlsx"), sheet= "Siblings") %>%
arrange(Date_of_Evaluation)
# Add  a Medical Record Row
Siblings.DOE.arranged <- Siblings.DOE.arranged %>%
mutate(MedicalRecord = ifelse(KBB_DD_status == "Yes", "Yes","No"))
# Save the data
write.xlsx(list(data = Siblings.DOE.arranged), file =  paste0(MatchedSibling_PW, "Final_ID_Tracker_send_to_A.xlsx"))
## Quality Control
sum(duplicated(Final.Data$Child_ID))
Final.Data$Child_ID[duplicated(Final.Data$Child_ID)]
# Load in data
Binded.data <- read_excel(paste0(FinalData_PW, "All Children.xlsx"))
# Compare ID's (should equal 0)
setdiff(Final.Data$Child_ID, Binded.data$Child_ID)
setdiff(Binded.data$Child_ID, Final.Data$Child_ID)
# Same size
length(Binded.data$Child_ID) == length(Final.Data$Child_ID)
# If not the same size check for duplicates in both dataset
Binded.data[duplicated(Binded.data$Child_ID),]
# Does every HOH Siblings have a DD and no DD pair?
Siblings %>%
group_by(HOH_ID) %>%
transmute(n = n_distinct(KBB_DD_status)) %>%
filter(n <2)
# Quality checker- a mismatch between DD and no DD
Siblings %>%
group_by(HOH_ID) %>%
count(HOH_ID) %>%
filter(n %% 2 != 0)
# Quality checker- checks for 4 children HOH where one is DD and three are not (vice versa)
Siblings %>%
group_by(HOH_ID) %>%
count(KBB_DD_status) %>%
pivot_wider(names_from = KBB_DD_status, values_from = n) %>%
mutate(mistmach = ifelse(No == Yes, "No", "Yes")) %>%
filter(mistmach == "Yes")
## Quality Control
sum(duplicated(Siblings$ID))
Siblings$ID[duplicated(Siblings$ID)]
# Compare ID's (should equal 0)
setdiff(1:length(Siblings$Child_ID), Siblings$ID)
# Any overall summary missing?
sum(is.na(Final.Data$Overall.Summary))
